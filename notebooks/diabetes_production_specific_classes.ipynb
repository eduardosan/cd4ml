{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experimentation cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes = load_diabetes(as_frame=True)\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "X_train = X.iloc[:300]\n",
    "X_test = X.iloc[300:]\n",
    "\n",
    "y_train = y.iloc[:300]\n",
    "y_test = y.iloc[300:]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df_input,scaler):\n",
    "    df = df_input.copy()\n",
    "    df.loc[(df_input['sex'] == -0.044642),['sex']] = 1\n",
    "    df.loc[df_input['sex'] != -0.044642,['sex']] = 0\n",
    "    df = scaler.transform(df)    \n",
    "    return df\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_features = preprocess_data(X_train,scaler=scaler)\n",
    "X_test_features = preprocess_data(X_test,scaler=scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ridge()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Ridge()\n",
    "model.fit(X_train_features,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53.410131367814344"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_pred=y_pred,y_true=y_test,squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Putting code into production format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Contract Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, OrderedDict,Union\n",
    "from cd4ml.task import Task\n",
    "from cd4ml.workflow import Workflow\n",
    "import pandas as pd\n",
    "\n",
    "class DataProcessor(ABC):\n",
    "    '''\n",
    "    Abstract class representing data processing steps that go into production.\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def load_data(self) -> Any:\n",
    "        '''Method to load the data into memory from a given path. Data should be stored in this class using the data property.'''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def preprocess(self,raw_data:Any)-> Any:\n",
    "        '''Method to preprocess the data. Data should be stored in this class using the data property.'''\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def raw_data(self):\n",
    "        return self._raw_data\n",
    "\n",
    "    @raw_data.setter\n",
    "    def raw_data(self,raw_data):\n",
    "        self._raw_data = raw_data\n",
    "\n",
    "    @property\n",
    "    def processed_data(self):\n",
    "        return self._processed_data\n",
    "\n",
    "    @processed_data.setter\n",
    "    def processed_data(self,processed_data):\n",
    "        self._processed_data = processed_data\n",
    "\n",
    "class FeatureGenerator(ABC):\n",
    "    '''\n",
    "    Abstract class representing feature generation process that goes into production.\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def get_features(self, data:pd.DataFrame)-> pd.DataFrame:\n",
    "        '''Method to generate feature data. Data should be stored in this class using the data property.'''\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_target(self, data:pd.DataFrame):\n",
    "        pass\n",
    "\n",
    "    @property\n",
    "    def features(self):\n",
    "        return self._features\n",
    "    \n",
    "    @features.setter\n",
    "    def features(self,features):\n",
    "        self._features = features\n",
    "        \n",
    "    @property\n",
    "    def target(self):\n",
    "        return self._target\n",
    "    \n",
    "    @target.setter\n",
    "    def target(self,target):\n",
    "        self._target = target\n",
    "\n",
    "@dataclass\n",
    "class Artifact:\n",
    "    name:str\n",
    "    object:Any\n",
    "    params:dict\n",
    "    path:str = ''\n",
    "\n",
    "class Model(ABC):\n",
    "    \n",
    "    def __init__(self, artifacts:Union[Artifact,list[Artifact]]) -> None:\n",
    "        '''\n",
    "        Base class representing the model contract that should be used to put a machine learning model into production\n",
    "        Any complex object that is used during training or prediction should be stored in the artifacts property.\n",
    "        .\n",
    "        '''\n",
    "        self.artifacts = artifacts\n",
    "        self._check_artifacts()\n",
    "\n",
    "    @abstractmethod\n",
    "    def fit(self,X,y):\n",
    "        '''\n",
    "        Method to train the machine learning model. \n",
    "        Any complex object that is used during training should be stored as an Artifact class.\n",
    "        ''' \n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def predict(self,X):\n",
    "        '''\n",
    "        Method to make predictions with the machine learning model.\n",
    "        Any complex object that is used during prediction should be retrieved from the artifacts property.\n",
    "        '''\n",
    "        pass    \n",
    "    \n",
    "    def _check_artifacts(self):\n",
    "        if self.artifacts is None or self.artifacts == []:\n",
    "            raise ValueError(\"A proper artifacts should be set in order to use the model.\")\n",
    "\n",
    "    @property\n",
    "    def artifacts(self)-> list[Artifact]:\n",
    "        '''\n",
    "        This property contains all object artifacts (Transformers,Estimators, etc) used during fit and that will be used during predict.\n",
    "        This is to ensure the proper logging into the experiment tracking tool.\n",
    "        '''\n",
    "        return self._artifacts\n",
    "\n",
    "    @artifacts.setter\n",
    "    def artifacts(self,artifacts:Union[Artifact,list[Artifact]]):\n",
    "        if isinstance(artifacts,list) and isinstance(artifacts[0],Artifact):\n",
    "            self._artifacts = artifacts\n",
    "        elif isinstance(artifacts,Artifact):\n",
    "            self._artifacts = [artifacts]\n",
    "        else:\n",
    "            raise TypeError(\"artifacts object should be a list of Artifact classes or a single Artifact.\")\n",
    "\n",
    "    @artifacts.getter\n",
    "    def artifacts_objects(self):\n",
    "        return [x.object for x in self._artifacts]\n",
    "\n",
    "    @artifacts.getter\n",
    "    def artifacts_params(self):\n",
    "        return [x.params for x in self._artifacts]\n",
    "\n",
    "class ArtifactsHandler(ABC):\n",
    "    \n",
    "    @abstractmethod\n",
    "    def save(artifacts:list[Artifact],path:str):\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load(parameters:dict,path:str)->list[Artifact]:\n",
    "        pass\n",
    "\n",
    "class ModelEvaluator:\n",
    "\n",
    "    def __init__(self,model:Model):\n",
    "        self.model = model\n",
    "\n",
    "    def evaluate(self,X,y,metrics:Union[Callable,list[Callable]]):\n",
    "\n",
    "        y_pred = self.model.predict(X) #Feature Improvement: some metrics might not work with prediction, but with prediction_proba.\n",
    "\n",
    "        metric_values = {}\n",
    "\n",
    "        for metric in metrics:\n",
    "            metric_name = metric.__name__\n",
    "            metric_values[metric_name] = metric(y_pred,y) \n",
    "        self.metrics = metric_values\n",
    "        \n",
    "        return self.metrics\n",
    "        \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self._metrics\n",
    "\n",
    "    @metrics.setter\n",
    "    def metrics(self,metrics:dict):\n",
    "        self._metrics = metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transforming experimentation into production code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing concrete classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import dump, load\n",
    "\n",
    "class DiabetesDataProcessor(DataProcessor):\n",
    "    \n",
    "    def __init__(self,loader) -> None:\n",
    "        self._loader = loader\n",
    "    \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        diabetes = self._loader(as_frame=True)\n",
    "        df = diabetes.data\n",
    "        df['target'] = diabetes.target\n",
    "        self.raw_data = df\n",
    "        return df\n",
    "\n",
    "    def preprocess(self,raw_data:pd.DataFrame) -> pd.DataFrame:\n",
    "        df = self._encode_sex_feature(raw_data)\n",
    "        self.processed_data = df\n",
    "        return df\n",
    "\n",
    "    def _encode_sex_feature(self,df:pd.DataFrame):\n",
    "        df_encoded = df.copy()\n",
    "        df_encoded.loc[(df['sex'] == -0.044642),['sex']] = 1\n",
    "        df_encoded.loc[df['sex'] != -0.044642,['sex']] = 0\n",
    "        return df_encoded\n",
    "\n",
    "class DiabetesFeatureGenerator(FeatureGenerator):\n",
    "    \n",
    "    def get_features(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        if 'target' in data.columns:\n",
    "            return data.drop(columns=['target'])\n",
    "        else:\n",
    "            return data\n",
    "        \n",
    "    def get_target(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        try:\n",
    "            return data['target']\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "class DiabetesModel(Model):\n",
    "    \n",
    "    def __init__(self, model, model_params, scaler, scaler_params) -> None:\n",
    "        self.model = model(**model_params)\n",
    "        self.scaler = scaler(**scaler_params)\n",
    "        super().__init__(artifacts = [\n",
    "                                Artifact(name='model',object = self.model,params = model_params),\n",
    "                                Artifact(name='scaler',object = self.scaler,params = scaler_params)])\n",
    "\n",
    "    def fit(self, X:pd.DataFrame, y:pd.DataFrame):\n",
    "        estimator,scaler = self.artifacts_objects\n",
    "        scaler.fit(X)\n",
    "        X_norm = scaler.transform(X)\n",
    "        estimator.fit(X_norm,y)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        estimator,scaler = self.artifacts_objects\n",
    "        X_norm = scaler.transform(X)\n",
    "        return estimator.predict(X_norm)\n",
    "\n",
    "class DiabetesArtifactHandler(ArtifactsHandler):\n",
    "    \n",
    "    def __init__(self,path) -> None:\n",
    "        self.path = path\n",
    "\n",
    "    def save(self,artifacts:list[Artifact]):\n",
    "        estimator, scaler = artifacts\n",
    "        dump(estimator.object, self.path+f'/{estimator.name}.joblib') \n",
    "        dump(scaler.object, self.path+f'/{scaler.name}.joblib')\n",
    "\n",
    "    def load(self,parameters:dict) -> list[Artifact]:\n",
    "        artifacts = []\n",
    "        \n",
    "        model_params = parameters['model_params']\n",
    "        estimator = load(self.path+'/model.joblib') \n",
    "        artifacts.append(Artifact(name='model',object=estimator,params=model_params))\n",
    "        \n",
    "        scaler_params = parameters['scaler_params']\n",
    "        scaler = load(self.path+'/scaler.joblib') \n",
    "        artifacts.append(Artifact(name='scaler',object=scaler,params=scaler_params))\n",
    "\n",
    "        return artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting parameters and loading dependencies\n",
    "\n",
    "All the necessary parameters and lib dependencies should be structured in a parameters.py file that will be passed to the ModelBuilding and ModelServing pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error,mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def rmse(y_true,y_pred):\n",
    "    return mean_squared_error(y_true,y_pred,squared=False)\n",
    "\n",
    "data_processor_params = {\n",
    "    'loader':load_diabetes\n",
    "}\n",
    "\n",
    "feature_generator_params = {\n",
    "}\n",
    "\n",
    "model_params = {\n",
    "    'model':Ridge,\n",
    "    'model_params':{'fit_intercept':True,'solver':'lsqr','alpha':0.5},\n",
    "    'scaler':MinMaxScaler,\n",
    "    'scaler_params':{'feature_range':[0,1]}\n",
    "}\n",
    "\n",
    "evaluator_params = {\n",
    "    'metrics':[rmse,mean_squared_error,mean_absolute_error]\n",
    "}\n",
    "\n",
    "artifacts_handler_parameters = {\n",
    "    'model_params':model_params['model_params'],\n",
    "    'scaler_params':model_params['scaler_params']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_processor = DiabetesDataProcessor(**data_processor_params)\n",
    "raw_data = data_processor.load_data()\n",
    "processed_data = data_processor.preprocess(raw_data)\n",
    "\n",
    "feature_generator = DiabetesFeatureGenerator(**feature_generator_params)\n",
    "X = feature_generator.get_features(processed_data)\n",
    "y = feature_generator.get_target(processed_data)\n",
    "\n",
    "# In a more realistic scenario, training and test sets will be chosen externally to this pipeline.\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.8,random_state=42)\n",
    "\n",
    "model = DiabetesModel(**model_params)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results: {'rmse': 54.96516990330306, 'mean_squared_error': 3021.1699024989725, 'mean_absolute_error': 43.681485821721225}\n",
      "\n",
      "Test results: {'rmse': 58.9462209485551, 'mean_squared_error': 3474.6569641158762, 'mean_absolute_error': 48.04694278124167}\n"
     ]
    }
   ],
   "source": [
    "evaluator = ModelEvaluator(model=model)\n",
    "evaluator_params['X'] = X_train\n",
    "evaluator_params['y'] = y_train\n",
    "results = evaluator.evaluate(**evaluator_params)\n",
    "print(\"Train results:\",results)\n",
    "\n",
    "evaluator_params['X'] = X_test\n",
    "evaluator_params['y'] = y_test\n",
    "results = evaluator.evaluate(**evaluator_params)\n",
    "print(\"\\nTest results:\",results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving Artifacts (This step is orchestrated by ModelBuilding steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = DiabetesArtifactHandler(path = \".\")\n",
    "handler.save(artifacts=model.artifacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Artifacts of a Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model,handler\n",
    "\n",
    "handler = DiabetesArtifactHandler(path = \".\")\n",
    "artifacts = handler.load(parameters=artifacts_handler_parameters)\n",
    "model = DiabetesModel(**model_params)\n",
    "model.artifacts = artifacts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Saved Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data_processor,feature_generator\n",
    "\n",
    "data_processor = DiabetesDataProcessor(**data_processor_params)\n",
    "raw_data = data_processor.load_data()\n",
    "processed_data = data_processor.preprocess(raw_data)\n",
    "del raw_data\n",
    "\n",
    "feature_generator = DiabetesFeatureGenerator(**feature_generator_params)\n",
    "X = feature_generator.get_features(processed_data)\n",
    "y = feature_generator.get_target(processed_data)\n",
    "del processed_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train results: {'rmse': 54.96516990330306, 'mean_squared_error': 3021.1699024989725, 'mean_absolute_error': 43.681485821721225}\n",
      "\n",
      "Test results: {'rmse': 58.9462209485551, 'mean_squared_error': 3474.6569641158762, 'mean_absolute_error': 48.04694278124167}\n"
     ]
    }
   ],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.8,random_state=42)\n",
    "evaluator = ModelEvaluator(model=model)\n",
    "evaluator_params['X'] = X_train\n",
    "evaluator_params['y'] = y_train\n",
    "results = evaluator.evaluate(**evaluator_params)\n",
    "print(\"Train results:\",results)\n",
    "\n",
    "evaluator_params['X'] = X_test\n",
    "evaluator_params['y'] = y_test\n",
    "results = evaluator.evaluate(**evaluator_params)\n",
    "print(\"\\nTest results:\",results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Pipeline with loaded Modeland Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([191.2987309 ,  80.83555847, 160.00279695, 130.20762935,\n",
       "       123.27026075, 105.44001573,  94.55850702, 157.08911378,\n",
       "       149.93666538, 170.82674973])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_processor = DiabetesDataProcessor(**data_processor_params)\n",
    "raw_data = data_processor.load_data()\n",
    "processed_data = data_processor.preprocess(raw_data)\n",
    "\n",
    "feature_generator = DiabetesFeatureGenerator(**feature_generator_params)\n",
    "X = feature_generator.get_features(processed_data)\n",
    "\n",
    "y_pred = model.predict(X)\n",
    "y_pred[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train - Using Workflow and Tasks paradigms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inputs do Matheus:\n",
    "# Requisito: MLFlow expects the folowing: model (sklearn models for now), metrics (dict), parameters(dict), tags (dict) optional.\n",
    "# Resolucão do problema do sklearn: I can inherit the Model class from the sklearn BaseEstimator.\n",
    "# Sugestão: Params is an yaml.\n",
    "# Sugestão: herdar a classe workflow e criar um MachineLearningWorkflow para treinar ou para orquestrar o pipeline do experimento (feature generation + model training)\n",
    "\n",
    "# Comentarios Lucas:\n",
    "# Penso que esse pipeline pode ser interessante montar com o workflow do Edu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from cd4ml.task import Task\n",
    "# from cd4ml.workflow import Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dummy_function():#TODO: How to make this callable task run a method of a class?\n",
    "#     pass\n",
    "\n",
    "# load_data = Task(name='load_data', task=dummy_function)\n",
    "# preprocess_data = Task(name='preprocess_data', task=dummy_function)\n",
    "# generate_features = Task(name='generate_features', task=dummy_function)\n",
    "# generate_target = Task(name='generate_target', task=dummy_function)\n",
    "# train_test_split = Task(name='train_test_split', task=dummy_function)\n",
    "# train_model = Task(name='train_model', task=dummy_function)\n",
    "# evaluate_model_on_train = Task(name='evaluate_model_on_train', task=dummy_function)\n",
    "# evaluate_model_on_test = Task(name='evaluate_model_on_test', task=dummy_function)\n",
    "# # last step: save model\n",
    "\n",
    "# w = Workflow()\n",
    "# w.add_task(load_data)\n",
    "# w.add_task(preprocess_data,dependency=['load_data'])\n",
    "# w.add_task(generate_features,dependency=['preprocess_data'])\n",
    "# w.add_task(generate_target,dependency=['preprocess_data'])\n",
    "# w.add_task(train_test_split,dependency=['generate_features','generate_target']) #Question: There is a dependency but no input parameters is passed. How does it work?\n",
    "# w.add_task(train_model,dependency=['train_test_split']) #Similar case here, but only a portion of inputs parameters is passed. X_train and y_train.\n",
    "# w.add_task(evaluate_model_on_train,dependency=['train_model'])\n",
    "# w.add_task(evaluate_model_on_test,dependency=['train_model'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_config = {\n",
    "#     # \"download_folha\": {\n",
    "#     #     'params': {'url': \"https://feeds.folha.uol.com.br/emcimadahora/rss091.xml\"},\n",
    "#     #     'output': 'download_folha'\n",
    "#     # },\n",
    "#     # \"download_g1\": {\n",
    "#     #     'params': {'url': \"https://g1.globo.com/rss/g1/\"},\n",
    "#     #     'output': 'download_g1'\n",
    "#     # },\n",
    "#     # \"download_g1_brasil\": {\n",
    "#     #     'params': {'url': \"https://g1.globo.com/rss/g1/brasil\"},\n",
    "#     #     'output': 'download_g1_brasil'\n",
    "#     # },\n",
    "#     # \"feeds_aggregate\": {\n",
    "#     #     'params': None,\n",
    "#     #     'output': 'feeds_aggregate'\n",
    "#     # },\n",
    "#     # \"preprocess\": {\n",
    "#     #     'params': None,\n",
    "#     #     'output': 'preprocess'\n",
    "#     # }\n",
    "# }\n",
    "\n",
    "# output = w.run(run_config=run_config, executor='local')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fba3b5e072295f01be074a5a38b56fd3f9a8b5b7274b5dee54e74765bfdf1822"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
